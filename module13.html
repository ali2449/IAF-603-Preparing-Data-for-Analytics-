<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 13: Introduction to Spark SQL - IAF 603</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&family=Source+Code+Pro:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- Highlight.js for code highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- MathJax CDN -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!-- Custom MathJax Configuration -->
    <script src="./js/mathjax.js"></script>
    <!-- Custom Styles -->
    <link rel="stylesheet" href="./css/styles.css">
</head>
<body>
    <!-- Navbar -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light shadow-sm sticky-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="./index.html">IAF 603: Preparing Data for Analytics</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="./index.html">Home</a>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                            Modules
                        </a>
                        <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                            <li><a class="dropdown-item" href="./module1.html">Module 1</a></li>
                            <li><a class="dropdown-item" href="./module2.html">Module 2</a></li>
                            <li><a class="dropdown-item" href="./module3.html">Module 3</a></li>
                            <li><a class="dropdown-item" href="./module4.html">Module 4</a></li>
                            <li><a class="dropdown-item" href="./module5.html">Module 5</a></li>
                            <li><a class="dropdown-item" href="./module6.html">Module 6</a></li>
                            <li><a class="dropdown-item" href="./module7.html">Module 7</a></li>
                            <li><a class="dropdown-item" href="./module8.html">Module 8</a></li>
                            <li><a class="dropdown-item" href="./module9.html">Module 9</a></li>
                            <li><a class="dropdown-item" href="./module10.html">Module 10</a></li>
                            <li><a class="dropdown-item" href="./module11.html">Module 11</a></li>
                            <li><a class="dropdown-item" href="./module12.html">Module 12</a></li>
                            <li><a class="dropdown-item" href="./module13.html">Module 13</a></li>
                            <li><a class="dropdown-item" href="./module14.html">Module 14</a></li>
                        </ul>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="./index.html#contact">Contact</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Main Content -->
    <div class="container my-5">
        <!-- Module Title -->
        <section class="text-center py-5 bg-white rounded shadow">
            <h1 class="display-4 fw-bold text-dark">Module 13: Introduction to Spark SQL</h1>
        </section>

        <!-- Module Content -->
        <section class="my-5">
            <div class="bg-white p-4 rounded shadow">
                <!-- Learning Objectives -->
                <h2 class="h3 fw-bold border-bottom pb-2 mb-4">Learning Objectives</h2>
                <p>Upon completion of this module, participants will be able to:</p>
                <ul>
                    <li>Understand the fundamental concepts of distributed computing and why it’s necessary for big data.</li>
                    <li>Describe the basic architecture of Apache Spark and the role of Spark SQL.</li>
                    <li>Differentiate between Spark SQL and traditional SQL, especially regarding schema-on-read.</li>
                    <li>Execute basic Spark SQL queries against data in a Microsoft Fabric Lakehouse.</li>
                </ul>

                <!-- Introduction to Spark SQL -->
                <h2 class="h3 fw-bold border-bottom pb-2 mb-4">Introduction to Spark SQL</h2>
                <p>As datasets grow from gigabytes to terabytes and petabytes, traditional single-node databases struggle to keep up. Distributed computing, where tasks are parallelized across multiple machines, is essential for handling big data. Apache Spark is the leading open-source framework for this, and Spark SQL is its module that allows standard SQL queries on massive datasets. This module bridges traditional SQL skills to distributed analytics, focusing on applying these skills in modern cloud environments like Microsoft Fabric. Spark SQL leverages the same ANSI SQL syntax you’ve mastered, but its execution model is optimized for scalability, making it ideal for large-scale data preparation tasks in fields like healthcare, business, or public policy.</p>
                <p>Spark SQL’s power lies in its ability to process vast, diverse datasets stored in data lakes, such as patient records for a Greensboro hospital or transaction logs for a retail chain. Unlike traditional databases, Spark SQL’s schema-on-read approach allows flexible data ingestion without predefined schemas, accommodating evolving data structures. This module equips you to transition from single-server SQL to distributed analytics, preparing you for real-world big data challenges where traditional tools fall short.</p>

                <!-- Core Concepts of Distributed Computing -->
                <h3 class="h4 fw-bold mb-3">Core Concepts of Distributed Computing</h3>
                <p>Distributed computing divides tasks across multiple computers (nodes) to process large datasets efficiently.</p>
                <ul>
                    <li><strong>Cluster:</strong> A group of computers (nodes or workers) that collaborate to process data.</li>
                    <li><strong>Distributed Storage:</strong> Data is split into chunks and stored across nodes’ drives (e.g., HDFS, Amazon S3, Microsoft OneLake).</li>
                    <li><strong>Parallel Processing:</strong> Queries are broken into tasks executed simultaneously across nodes, with results aggregated for the final output.</li>
                </ul>
                <p>These concepts enable Spark to handle massive datasets by distributing both storage and computation. For example, analyzing years of patient data across a hospital network requires parallel processing to deliver results quickly, which single-node systems cannot achieve efficiently.</p>

                <!-- Spark Architecture and Spark SQL -->
                <h3 class="h4 fw-bold mb-3">Spark Architecture and Spark SQL</h3>
                <p>Spark’s architecture is designed for distributed processing, with Spark SQL as a high-level interface.</p>
                <ul>
                    <li><strong>Spark Driver:</strong> The master node that plans queries, breaks them into tasks, and schedules execution on worker nodes.</li>
                    <li><strong>Executors:</strong> Processes on worker nodes that execute tasks and return results to the driver.</li>
                    <li><strong>Spark SQL:</strong> Uses the Catalyst optimizer to create efficient execution plans, running SQL queries across the cluster.</li>
                </ul>
                <p>The Catalyst optimizer translates SQL into an optimized physical plan, distributing tasks to maximize parallelism. This makes Spark SQL ideal for complex queries on large datasets, such as aggregating sales data across multiple stores or analyzing sensor data from IoT devices.</p>

                <!-- Spark SQL vs. Traditional SQL -->
                <h3 class="h4 fw-bold mb-3">Spark SQL vs. Traditional SQL</h3>
                <p>While Spark SQL uses familiar ANSI SQL syntax, its execution model differs significantly from traditional SQL databases.</p>
                <ul>
                    <li><strong>Schema-on-Write (Traditional):</strong> Requires a predefined schema (<code>CREATE TABLE</code>) before loading data, with validation at write time.</li>
                    <li><strong>Schema-on-Read (Spark SQL):</strong> Infers schema when reading data from files (e.g., CSV, JSON, Parquet), offering flexibility for unstructured or evolving data.</li>
                    <li><strong>Data Formats:</strong> Optimized for columnar formats like Parquet and Delta Lake, which reduce I/O by reading only required columns.</li>
                </ul>
                <p>Schema-on-read eliminates the need for upfront schema design, making it easier to work with semi-structured data like JSON from APIs. Parquet’s columnar storage speeds up analytical queries by skipping irrelevant data, crucial for big data workloads.</p>

                <!-- Running Spark SQL in Microsoft Fabric -->
                <h3 class="h4 fw-bold mb-3">Running Spark SQL in Microsoft Fabric</h3>
                <p>Microsoft Fabric provides a user-friendly environment for Spark SQL, typically within Notebooks in a Lakehouse architecture.</p>
                <ul>
                    <li><strong>Lakehouse:</strong> Combines data lake flexibility (storing all data types) with data warehouse features (ACID transactions, optimized querying). Data is often stored in Delta Lake format.</li>
                    <li><strong>Notebooks:</strong> Interactive web-based environments for writing and executing Spark SQL, Python, or R code, with results displayed inline.</li>
                </ul>
                <p><strong>Sample Data (survey_responses_delta Table):</strong></p>
                <table class="table table-bordered">
                    <thead>
                        <tr>
                            <th>response_id</th>
                            <th>city</th>
                            <th>state</th>
                            <th>satisfaction_score</th>
                            <th>response_date</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1</td>
                            <td>Greensboro</td>
                            <td>NC</td>
                            <td>4</td>
                            <td>2025-01-10</td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>Raleigh</td>
                            <td>NC</td>
                            <td>3</td>
                            <td>2025-01-11</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>Richmond</td>
                            <td>VA</td>
                            <td>5</td>
                            <td>2025-01-12</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>Charleston</td>
                            <td>SC</td>
                            <td>2</td>
                            <td>2025-01-13</td>
                        </tr>
                        <tr>
                            <td>5</td>
                            <td>Greensboro</td>
                            <td>NC</td>
                            <td>4</td>
                            <td>2025-01-14</td>
                        </tr>
                    </tbody>
                </table>
                <div class="code-block">
                    <button class="btn btn-sm btn-secondary copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code class="language-sql">-- Run in a Fabric Notebook cell with %sql
-- Aggregate survey responses by city and state
%sql
SELECT
    city,
    state,
    AVG(satisfaction_score) AS avg_score,
    COUNT(*) AS num_responses
FROM
    my_lakehouse.survey_responses_delta
WHERE
    state IN ('NC', 'VA', 'SC')
GROUP BY
    city, state
HAVING
    COUNT(*) > 100
ORDER BY
    avg_score DESC;</code></pre>
                </div>
                <p><strong>Explanation:</strong> This query uses standard SQL syntax to aggregate survey data in a Fabric Lakehouse. Behind the scenes, the Spark Driver distributes tasks across executors, each processing a portion of the <code>survey_responses_delta</code> table (stored as Parquet in OneLake). Results are combined and displayed in the Notebook, transparent to the user. This is ideal for analyzing large-scale data, such as public health surveys across multiple states.</p>
                <p><strong>Expanded Example (Clickstream Analysis):</strong></p>
                <table class="table table-bordered">
                    <thead>
                        <tr>
                            <th>user_id</th>
                            <th>event_timestamp</th>
                            <th>page_viewed</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>101</td>
                            <td>2025-01-10 10:00:00</td>
                            <td>Homepage</td>
                        </tr>
                        <tr>
                            <td>101</td>
                            <td>2025-01-10 10:05:00</td>
                            <td>Product</td>
                        </tr>
                        <tr>
                            <td>102</td>
                            <td>2025-01-11 12:00:00</td>
                            <td>Homepage</td>
                        </tr>
                        <tr>
                            <td>103</td>
                            <td>2025-01-11 14:00:00</td>
                            <td>Checkout</td>
                        </tr>
                        <tr>
                            <td>102</td>
                            <td>2025-01-12 09:00:00</td>
                            <td>Product</td>
                        </tr>
                    </tbody>
                </table>
                <div class="code-block">
                    <button class="btn btn-sm btn-secondary copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code class="language-sql">-- Run in a Fabric Notebook cell with %sql
-- Count unique visitors per day
%sql
SELECT
    DATE(event_timestamp) AS event_date,
    COUNT(DISTINCT user_id) AS unique_visitors
FROM
    my_lakehouse.clickstream_logs
GROUP BY
    DATE(event_timestamp)
ORDER BY
    event_date;</code></pre>
                </div>
                <p><strong>Explanation:</strong> This query aggregates daily unique visitors from web logs, leveraging Spark SQL’s ability to process large-scale clickstream data efficiently. The distributed execution ensures scalability, making it suitable for analyzing website traffic for a large e-commerce platform.</p>

                <!-- Advanced Spark SQL with Window Functions -->
                <h3 class="h4 fw-bold mb-3">Advanced Spark SQL with Window Functions</h3>
                <p>Spark SQL supports advanced features like window functions, enabling complex analytics on distributed data.</p>
                <div class="code-block">
                    <button class="btn btn-sm btn-secondary copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code class="language-sql">-- Run in a Fabric Notebook cell with %sql
-- Calculate running total of sales per product category
%sql
SELECT
    category,
    DATE_TRUNC('month', sale_date) AS sale_month,
    SUM(sale_amount) AS monthly_sales,
    SUM(SUM(sale_amount)) OVER (PARTITION BY category ORDER BY DATE_TRUNC('month', sale_date)) AS running_total
FROM
    my_lakehouse.sales_data
GROUP BY
    category, DATE_TRUNC('month', sale_date)
ORDER BY
    category, sale_month;</code></pre>
                </div>
                <p><strong>Explanation:</strong> This query uses a window function to compute running totals of sales per product category, executed across a distributed dataset. Spark’s parallel processing ensures efficiency, ideal for financial reporting in retail analytics.</p>
                <p><strong>Expanded Example (Patient Readmissions):</strong></p>
                <table class="table table-bordered">
                    <thead>
                        <tr>
                            <th>patient_id</th>
                            <th>admit_date</th>
                            <th>diagnosis</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1001</td>
                            <td>2025-01-10</td>
                            <td>Influenza</td>
                        </tr>
                        <tr>
                            <td>1001</td>
                            <td>2025-02-15</td>
                            <td>Pneumonia</td>
                        </tr>
                        <tr>
                            <td>1002</td>
                            <td>2025-01-20</td>
                            <td>Fracture</td>
                        </tr>
                        <tr>
                            <td>1001</td>
                            <td>2025-03-10</td>
                            <td>Influenza</td>
                        </tr>
                    </tbody>
                </table>
                <div class="code-block">
                    <button class="btn btn-sm btn-secondary copy-btn" onclick="copyCode(this)">Copy</button>
                    <pre><code class="language-sql">-- Run in a Fabric Notebook cell with %sql
-- Calculate time between readmissions
%sql
WITH PatientAdmissions AS (
    SELECT
        patient_id,
        admit_date,
        LAG(admit_date) OVER (PARTITION BY patient_id ORDER BY admit_date) AS previous_admit
    FROM
        my_lakehouse.patient_admissions
)
SELECT
    patient_id,
    admit_date,
    previous_admit,
    DATEDIFF(admit_date, previous_admit) AS days_between_admissions
FROM
    PatientAdmissions
WHERE
    previous_admit IS NOT NULL;</code></pre>
                </div>
                <p><strong>Explanation:</strong> This query uses a CTE and window function in Spark SQL to calculate days between hospital readmissions, leveraging distributed processing for large patient datasets. It’s critical for healthcare analytics, such as tracking readmission rates.</p>

                <!-- Assessments -->
                <h2 class="h3 fw-bold border-bottom pb-2 mb-4">Assessments</h2>
                <ol>
                    <li><strong>Conceptual Question:</strong> What does "schema-on-read" mean, and why is it a significant advantage when working with diverse or evolving datasets in a data lake?</li>
                    <li><strong>Conceptual Scenario:</strong> You need to calculate total sales for the last year from a 10-terabyte dataset of transaction logs stored as Parquet files. Explain why Spark SQL would be more appropriate than a traditional PostgreSQL database on a single server.</li>
                    <li><strong>Coding Task:</strong> Write a Spark SQL query to find the number of unique visitors per day from a <code>clickstream_logs</code> table with columns <code>user_id</code> and <code>event_timestamp</code>, stored in a Fabric Lakehouse.</li>
                    <li><strong>Expanded Task (Customer Segmentation):</strong> Using a <code>customer_transactions</code> table with columns <code>customer_id</code>, <code>transaction_date</code>, and <code>amount</code>, write a Spark SQL query to identify customers with total purchases above $10,000 in the last 6 months, grouped by month.</li>
                </ol>
            </div>
        </section>
    </div>

    <!-- Footer -->
    <footer class="bg-light py-3 text-center">
        <p class="text-muted mb-0">&copy; 2025 Ali Noori, UNCG. All rights reserved.</p>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js" integrity="sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN" crossorigin="anonymous"></script>
    <!-- Highlight.js Initialization -->
    <script>hljs.highlightAll();</script>
    <!-- Custom Copy Functionality -->
    <script>
        function copyCode(button) {
            const code = button.nextElementSibling.querySelector('code');
            navigator.clipboard.writeText(code.innerText).then(() => {
                button.textContent = 'Copied!';
                setTimeout(() => { button.textContent = 'Copy'; }, 2000);
            });
        }
    </script>
</body>
</html>